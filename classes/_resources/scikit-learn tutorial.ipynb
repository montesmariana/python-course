{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "086359b9",
   "metadata": {},
   "source": [
    "This notebook follows the [scikit-learn text analytics tutorial](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html), albeit reordering some cells and adding some explanations.\n",
    "\n",
    "We first will load all the packages and modules used in the tutorial (which imports them one by one as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26121a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # to compute accuracy\n",
    "import pandas as pd # to print nicer tables\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups # get dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer # for preprocessing, frequencies\n",
    "\n",
    "# classification algorithms\n",
    "from sklearn.naive_bayes import MultinomialNB # naive bayes\n",
    "from sklearn.linear_model import SGDClassifier # support vector machine\n",
    "\n",
    "# evaluate\n",
    "from sklearn import metrics\n",
    "\n",
    "# streamline code\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a64b4c",
   "metadata": {},
   "source": [
    "# Get dataset\n",
    "\n",
    "The dataset is a part of the \"20 Newsgroups\" built-in dataset. Many packages for machine learning and data science come with built-in datasets for tests and tutorials.\n",
    "In this case, the dataset can be retrieved with the `sklearn.datasets.fetch_20newsgroups()` function. Let's check out the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706c5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(fetch_20newsgroups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f5642",
   "metadata": {},
   "source": [
    "The tutorial provides a list of 4 out of the 20 categories for a partial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c9e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c7e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twenty_train.DESCR) # get full documentation of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dd46ba",
   "metadata": {},
   "source": [
    "As indicated in the documentation, `twenty_train` has a few interesting attributes:\n",
    "\n",
    "- `data` = the contents\n",
    "- `filenames` = the paths to the files where the contents are stored\n",
    "- `target` = the index of the category that each element belongs to\n",
    "\n",
    "These three attributes have the same length. For a given index `i`,\n",
    "the filename of `twenty_train.data[i]` is `twenty_train.filenames[i]`,\n",
    "and its category is `twenty_train.target_names[twenty_train.target[i]]`.\n",
    "\n",
    "**What's the point of `target`?**\n",
    "In machine learning, the idea of a **classifier** is that you build a model that can predict the category that something belongs to. You build the model by studying a **training** dataset, learning patterns from it, and then applying them to unseen **test** data.\n",
    "A **supervised learning algorithm** requires the training dataset to be labelled with the categories that you want to learn. In this case, we have thousands of items and for each of them we know whether they belong to the category \"atheism\", \"computer graphics\", \"medicine\" or \"Christanity\". The idea is that the model learns the patterns that characterize the texts from each category so that, given a new text from the test dataset, it can reliably classify it in one of those four categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d499213",
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7fabe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(twenty_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110999ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c734bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869c345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twenty_train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e49e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b478fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.target_names[twenty_train.target[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f036623",
   "metadata": {},
   "source": [
    "# BOW representation of the text\n",
    "With simple text, all that a computer sees of the words are their characters. However, the similarity between texts based on their _characters_ is not very informative.\n",
    "Instead, what we do is represent a word as a sequence of numbers (a **vector**), each number representing the frequency of that word in a given document. This also in turns represents a document as a vector, each item representing the frequency of a given word in it. The idea is that documents that have the same word occurring in it with a similar frequency are similar to each other.\n",
    "\n",
    "This will be represented as a **sparse matrix**: a table of numbers that has mostly zeros (because most words do not occur in most documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee1f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d2e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape # a matrix with one row per document and one column per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd1840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm') # frequency of 'algorithm' in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef6fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581705e6",
   "metadata": {},
   "source": [
    "We can then transform the raw frequencies into **tf-idf** (Term frequency x Inverse Document Frequency) so that:\n",
    "- The frequencies are relative to the size of the document\n",
    "- Words that occur in many documents have less weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe25ad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7af4311",
   "metadata": {},
   "source": [
    "# Train a classifier\n",
    "For this tutorial, a naÃ¯ve Bayes classifier is used.\n",
    "\n",
    "The previous functions, which just transformed the data, only needed the data itself.\n",
    "In this case, instead, we need to elements for training: the (transformed) data and the labels: the `i`th label corresponds to the `i`th row of the data matrix, which is a numerical representation of the document used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4071df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b12e0ae",
   "metadata": {},
   "source": [
    "We can now use our model `clf` to **predict** the label of new data, such as the strings below in `docs_new`.\n",
    "First, we have to transform this new data in the same way, using the tokenizer `count_vect` and the `tfidf_transformer`... although we don't need to _fit_ them anymore, just transform the data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38fbfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "X_new_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa48cc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_present = [i for i in range(X_new_tfidf.shape[1]) if sum(X_new_tfidf.toarray()[:,i]) > 0]\n",
    "words_present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d85132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw counts of the words in the mini corpus\n",
    "pd.DataFrame(X_new_counts.toarray()[:,words_present],\n",
    "            columns = count_vect.get_feature_names_out()[words_present])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add04c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf of the words in the mini corpus\n",
    "pd.DataFrame(X_new_tfidf.toarray()[:,words_present],\n",
    "            columns = count_vect.get_feature_names_out()[words_present])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88759edf",
   "metadata": {},
   "source": [
    "Once we have transformed the test data we can apply our model to try to predict the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print(f'{doc} => {twenty_train.target_names[category]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17241465",
   "metadata": {},
   "source": [
    "For this tiny test set, it seems alright!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569480c",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "If you need to run this workflow many times you can make it more compact by using a pipeline, which sequentially applies a list of transformations (such as `CountVectorizer` and `TfidfTransformer`) ad then a final estimator (in this case `MultinomialNB`).\n",
    "\n",
    "Why would you want to run this many times? Because, even though we didn't see this, the transformations and the estimator have _hyperparameters_ which can return different results. You might want to explore different values of these hyperparameters to see which combination returns the best result, thus **tuning your model**. In that case, a pipeline streamlines your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15dd62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1813638",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_via_pipeline = text_clf.predict(docs_new)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted_via_pipeline):\n",
    "    print(f'{doc} => {twenty_train.target_names[category]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd27707",
   "metadata": {},
   "source": [
    "As you can see, one line of code sufficed to apply all transformations _and_ training to the train data, as well as all transforamtions _and_ prediction on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1532466b",
   "metadata": {},
   "source": [
    "# Evaluate performance\n",
    "Here we only tested on two sentences that were, in addition, designed to be very easy to classify. If we use a larger test dataset, looking at the matches manually to evaluate the model can be time consuming and hard to assess.\n",
    "\n",
    "The tutorial suggests a very simple measure of _accuracy_, i.e. the proportion of labels that the model got right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066ed792",
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42) # retrieve test data\n",
    "docs_test = twenty_test.data # extract the data part\n",
    "predicted = text_clf.predict(docs_test) # predict with the train model\n",
    "[(a, b, a == b) for a, b in zip(twenty_test.target, predicted)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14149cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d389bb",
   "metadata": {},
   "source": [
    "As a following step, the tutorial shows how to use a diferent kind of classification algorithm: a support vector machine. The chunks below does the same thing we did above but with a different algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c09b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=5, tol=None)),\n",
    "])\n",
    "\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd133e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a7f1d",
   "metadata": {},
   "source": [
    "With some functions from `sklearn.metrics` we can also look into the performance in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28a8751",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0789a4aa",
   "metadata": {},
   "source": [
    "In the confusion matrix, the rows indicate the true result, and the columns the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da5aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(metrics.confusion_matrix(twenty_test.target, predicted), columns=categories, index=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c15a39",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "\n",
    "As mentioned before, we might want to try out different values for hyperparameters that could affect our model. A grid search looks at all possible combinations of values that you provide.\n",
    "\n",
    "This is done by providing the pipeline and a dictionary of parameters to `sklearn.model_selection.GridSearchCV`. This dictionary has the hyperparameter names as keys, with the format `<pipeline-item>__<argument>`, and a list or tuple of possible arguments. The one below, for example, requests:\n",
    "\n",
    "- The values `(1, 1)` and `(1, 2)` for the `ngram_range` argument of `CountVectorizer()`, which is the `vect` element of the Pipeline. These values generate monograms and bigrams respectively.\n",
    "- The values `True` and `False` for the `use_idf` argument of `TfidfTransformer()`, which is the `tfidf` element of the Pipeline. These values switch between including IDF smoothing or not in the transformation of the frequencies.\n",
    "- The values `0.01` and `0.001` for the `alpha` argument of `SGDClassifier()`, which is the `clf` element of the Pipeline. This is a penalty parameter of the algorithm.\n",
    "\n",
    "The `cv` argument defines the number of folds for cross-validation. This means that the training set is split in 5 equal pieces and the train-test workflow is done 5 times _on each combination of hyperparameters_, with one of the splits as test set and the other 4 as training set.\n",
    "\n",
    "Finally, `n_jobs = -1` tells the computer to use as many CPU cores as we have available to run this computationally expensive task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f64c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf__alpha': (1e-2, 1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed5f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6ac341",
   "metadata": {},
   "source": [
    "Because it can take so long with the full training dataset, the tutorial suggests looking at the first 400 documents only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10cf1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b95f311",
   "metadata": {},
   "source": [
    "We do get as a result a classifier that we can use to predict - the difference with the previous one is that it has been tuned on a variety of hyperparameters and training-test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd6096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.target_names[gs_clf.predict(['God is love'])[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e8c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf.best_score_ # the score of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26f943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf.best_params_ # the hyperparameters of the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2e3e9",
   "metadata": {},
   "source": [
    "Finally, the `cv_results_` attribute gives us the details of the result with a matrix with one row per combination of hyperparameters and columns for different properties, such as the time it took to test it, the value of each parameter, the score of each test and a summary of these scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db9b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gs_clf.cv_results_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
